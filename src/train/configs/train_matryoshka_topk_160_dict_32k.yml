
name: matryoshka_top_160_dict_32768
# optimization parameters
seed: 49
batch_size: 4096
lr: 0.003
l1_coeff: 0
beta1: 0.9
beta2: 0.99
max_grad_norm: 100000
num_epochs: 3
wandb_name: "sae_matryoshka_top_160"
train_cache_size: 8000000
val_data_size: 1000000

# model parameters
sae_type: matryoshka_sae
dtype: torch.bfloat16
model_dtype: torch.bfloat16
n_batches_to_dead: 20
act_size: 4096
top_k: 160
top_k_aux: 512
aux_penalty: 0.03125
input_unit_norm: false
group_sizes: [512, 512, 1024, 2048, 4096, 8192, 16384]
dict_size: 32768

# training setup
dataset_path: /root/data/t5_embedding_gpu.zarr/
results_path: /s3-data/usr/dgaribi/TokenSlidersCKPT/

perf_log_freq: 100
checkpoint_freq: 10000
data_loader_workers: 1

bandwidth: 0.001